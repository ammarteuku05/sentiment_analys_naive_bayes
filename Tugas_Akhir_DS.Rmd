---
title: "Tugas Akhir Data Science"
author: "ammar&rifqy"
date: "20/01/2021"
output: html_document
runtime: shiny
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Library & Set Credential
```{r}
library(twitteR)
library(ROAuth)

access_secret<-"WjSUnincPjzNqpkGfecbGHoiYus5OfgoAMwdqiJ9tQQhW"
access_token<-"748334852324339713-LFxOotoZ70gDxbMGOvqh1cFlGUMQRwA"
consumer_key<-"umVYxzBgCgpeqcOgjOg3V0U2Y"
consumer_secret<-"m5DzETLWti8QEBsGyzoAychSfzINnFHSgMX6llUqumimMRBo0q"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

Search Data
```{r}
  tweets_data = searchTwitter("Covid+Pemerintah", n=1000, lang="id")
```

Cleaning Dataset
```{r}
tweets_data_pembersihan <- sapply(tweets_data, function(x) x$getText())

catch.error = function(x)
{
  y = NA
  
  catch_error = tryCatch(tolower(x), error=function(e) e)
  
  if (!inherits(catch_error, "error"))
    y = tolower(x)
  
  return(y)
}

cleanTweets <- function(tweet) {
  #remove html links:
  tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
  #remove retweet entities:
  tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
  #remove #hashtags:
  tweet = gsub("#\\w+", " ", tweet)
  #remove all "@people":
  tweet = gsub("@\\w+", " ", tweet)
  #remove all punctuations:
  tweet = gsub("[[:punct:]]", " ", tweet)
  #remove numbers, kita hanya butuh teks untuk analytics
  tweet = gsub("[[:digit:]]", " ", tweet)
  #remove unnecessary spaces (white spaces, tabs, etc)
  tweet = gsub("[ \t]{2,}", " ", tweet)
  tweet = gsub("^\\s+|\\s+$", "", tweet)
  #remove amp
  tweet = gsub("&amp", " ", tweet)
  #remove crazy character
  tweet = gsub("[^a-zA-Z0-9]", " ", tweet)
  #remove alphanumeric
  tweet = gsub("[^[:alnum:]]", " ", tweet) 
  
  #ubah semua kata menjadi lowercase:
  tweet = catch.error(tweet)
  tweet
}

cleanTweetsAndRemoveNAs <- function(Tweets) {
  TweetsCleaned = sapply(Tweets, cleanTweets)
  #remove "NA" tweets:
  TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
  names(TweetsCleaned) = NULL
  #remove repetitive tweets:
  TweetsCleaned = unique(TweetsCleaned)
  TweetsCleaned
}
cleaned_tweets = cleanTweetsAndRemoveNAs(tweets_data_pembersihan)
write.csv(cleaned_tweets, "tabel_baru.csv")
```

## Bagan/Scarletplot
```{r bagi data}
library(e1071)
library(caret)
library(syuzhet)
#digunakan untuk membaca file csv yang sudah di cleaning data 
datanya<-read.csv("tabel_baru.csv",stringsAsFactors = FALSE)
#digunakan untuk mengeset variabel cloumn text menjadi char
review <-as.character(datanya$x)
#Calls the NRC sentiment dictionary to calculate the presence of eight different emotions and their corresponding valence in a text file.
get_nrc_sentiment('happy')
get_nrc_sentiment('excitement')
s<-get_nrc_sentiment(review)
review_combine<-cbind(datanya$x)
par(mar=rep(3,4))
a<- barplot(colSums(s),col=rainbow(10),ylab='count',main='sentiment analisis')
iki_ba <- a
```

##WordCloud
```{r}
  library(tm)
  library(RTextTools)
  library(e1071)
  library(dplyr)
  library(caret)
  library(SnowballC)
  df<-read.csv("tabel_baru.csv",stringsAsFactors = FALSE)
  glimpse(df)
  #Set the seed of Râ€˜s random number generator, which is useful for creating simulations or random objects that can be reproduced.
  set.seed(20)
  df<-df[sample(nrow(df)),]
  df<-df[sample(nrow(df)),]
  glimpse(df)
  corpus<-Corpus(VectorSource(df$x))
  corpus
  inspect(corpus[1:10])
  #fungsinya untuk membersihkan data data yang tidak dibutuhkan 
  corpus.clean<-corpus%>%
      tm_map(content_transformer(tolower))%>%
      tm_map(removePunctuation)%>%
      tm_map(removeNumbers)%>%
      tm_map(removeWords,stopwords())%>%
      tm_map(stripWhitespace)
  dtm<-DocumentTermMatrix(corpus.clean)
  inspect(dtm[1:10,1:20])
  df.train<-df[1:50,]
  df.test<-df[51:100,]
  dtm.train<-dtm[1:50,]
  dtm.test<-dtm[51:100,]
  corpus.clean.train<-corpus.clean[1:50]
  corpus.clean.test<-corpus.clean[51:100]
  dim(dtm.train)
  fivefreq<-findFreqTerms(dtm.train,5)
  length(fivefreq)
  dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
  #dim(dtm.train.nb)
  dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
  dim(dtm.test.nb)
   
  convert_count <- function(x){
      y<-ifelse(x>0,1,0)
      y<-factor(y,levels=c(0,1),labels=c("no","yes"))
      y
  }
  trainNB<-apply(dtm.train.nb,2,convert_count)
  testNB<-apply(dtm.test.nb,1,convert_count)
  library(wordcloud)
  wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
```
##Shiny

```{r}
library(shiny)
library(here)
library(vroom)
library(dplyr)
library(ggplot2)
library(plotly)
library(syuzhet)
twitter<- vroom(here("archive_4","tabel_baru.csv"))
tweet<- twitter$text
ui <- fluidPage(
    titlePanel("Analisa"),
        mainPanel(
            
            tabsetPanel(type = "tabs",
                        tabPanel("Bagan", plotOutput("scatterplot")), 
                        # Plot
                        tabPanel("Data", DT::dataTableOutput('tbl')), 
                        # Output Data Dalam Tabel
                        tabPanel("Wordcloud", plotOutput("Wordcloud"))
                        )
        )
    )
server <- function(input, output) {
    
    # Output Data
    output$tbl = DT::renderDataTable({
        DT::datatable(twitter, options = list(lengthChange = FALSE))
    })
    
    output$scatterplot <- renderPlot({produk_dataset<-read.csv("tabel_baru.csv",stringsAsFactors = FALSE)
review <-as.character(produk_dataset$x)
get_nrc_sentiment('happy')
get_nrc_sentiment('excitement')
s<-get_nrc_sentiment(review)
review_combine<-cbind(produk_dataset$x,s)
par(mar=rep(3,4))
barplot(colSums(s),col=rainbow(10),ylab='count',main='sentiment analisis')
    }, height=400)
    output$Wordcloud <- renderPlot({
   set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)
corpus<-Corpus(VectorSource(df$x))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan 
corpus.clean<-corpus%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords,stopwords())%>%
    tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train<-df[1:50,]
df.test<-df[51:100,]
dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]
corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]
dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
 
convert_count <- function(p){
    y<-ifelse(p>0,1,0)
    y<-factor(y,levels=c(0,1),labels=c("no","yes"))
    y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)
library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
  })
}
shinyApp(ui = ui, server = server)
```

